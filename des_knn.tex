%% 
%% Copyright 2019-2020 Elsevier Ltd
%% 
%% This file is part of the 'CAS Bundle'.
%% --------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'CAS Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for cas-dc documentclass for 
%% double column output.

%\documentclass[a4paper,fleqn,longmktitle]{cas-dc}
\documentclass[a4paper,fleqn]{cas-dc}

%\usepackage[numbers]{natbib}
%\usepackage[authoryear]{natbib}
\usepackage[authoryear,longnamesfirst]{natbib}
%% The amssymb package provides various useful mathematical symbols
\usepackage{amssymb}
%% The amsmath package provides various useful equation environments.
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bbm}
% \usepackage{algorithmic}

%%%Author definitions
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
\tsc{EP}
\tsc{PMS}
\tsc{BEC}
\tsc{DE}
%%%

% Uncomment and use as if needed
%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}[theorem]{Lemma}
%\newdefinition{rmk}{Remark}
%\newproof{pf}{Proof}
%\newproof{pot}{Proof of Theorem \ref{thm}}

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}

% Short title
\shorttitle{Dynamic Early Stopping for KNN}

% Short author
\shortauthors{s Jang et~al.}

% Main title of the paper
\title [mode = title]{DES-kNN: Dynamic Early Stopping for k-Nearest Neighbor Search with a Beta-Geometric Guarantee}
% Title footnote mark
% eg: \tnotemark[1]
\tnotemark[1,2]

% Title footnote 1.
% eg: \tnotetext[1]{Title footnote text}
% \tnotetext[<tnote number>]{<tnote text>} 
\tnotetext[1]{This document is the results of the research
   project funded by the National Science Foundation.}

\tnotetext[2]{The second title footnote which is a longer text matter
   to fill through the whole text width and overflow into
   another line in the footnotes area of the first page.}


% First author
%
% Options: Use if required
% eg: \author[1,3]{Author Name}[type=editor,
%       style=chinese,
%       auid=000,
%       bioid=1,
%       prefix=Sir,
%       orcid=0000-0000-0000-0000,
%       facebook=<facebook id>,
%       twitter=<twitter id>,
%       linkedin=<linkedin id>,
%       gplus=<gplus id>]
\author[1,3]{CV Radhakrishnan}[type=editor,
                        auid=000,bioid=1,
                        prefix=Sir,
                        role=Researcher,
                        orcid=0000-0001-7511-2910]

% Corresponding author indication
\cormark[1]

% Footnote of the first author
\fnmark[1]

% Email id of the first author
\ead{cvr_1@tug.org.in}

% URL of the first author
\ead[url]{www.cvr.cc, cvr@sayahna.org}

%  Credit authorship
\credit{Conceptualization of this study, Methodology, Software}

% Address/affiliation
\affiliation[1]{organization={Elsevier B.V.},
    addressline={Radarweg 29}, 
    city={Amsterdam},
    % citysep={}, % Uncomment if no comma needed between city and postcode
    postcode={1043 NX}, 
    % state={},
    country={The Netherlands}}

% Second author
\author[2,4]{Han Theh Thanh}[style=chinese]

% Third author
\author[2,3]{CV Rajagopal}[%
   role=Co-ordinator,
   suffix=Jr,
   ]
\fnmark[2]
\ead{cvr3@sayahna.org}
\ead[URL]{www.sayahna.org}

\credit{Data curation, Writing - Original draft preparation}

% Address/affiliation
\affiliation[2]{organization={Sayahna Foundation},
    % addressline={}, 
    city={Jagathy},
    % citysep={}, % Uncomment if no comma needed between city and postcode
    postcode={695014}, 
    state={Trivandrum},
    country={India}}

% Fourth author
\author%
[1,3]
{Rishi T.}
\cormark[2]
\fnmark[1,3]
\ead{rishi@stmdocs.in}
\ead[URL]{www.stmdocs.in}

\affiliation[3]{organization={STM Document Engineering Pvt Ltd.},
    addressline={Mepukada}, 
    city={Malayinkil},
    % citysep={}, % Uncomment if no comma needed between city and postcode
    postcode={695571}, 
    state={Trivandrum},
    country={India}}

% Corresponding author text
\cortext[cor1]{Corresponding author}
\cortext[cor2]{Principal corresponding author}

% Footnote text
\fntext[fn1]{This is the first author footnote. but is common to third
  author as well.}
\fntext[fn2]{Another author footnote, this is a very long footnote and
  it should be a really long footnote. But this footnote is not yet
  sufficiently long enough to make two lines of footnote text.}

% For a title note without a number/mark
\nonumnote{This note has no numbers. In this work we demonstrate $a_b$
  the formation Y\_1 of a new type of polariton on the interface
  between a cuprous oxide slab and a polystyrene micro-sphere placed
  on the slab.
  }

% Here goes the abstract
\begin{abstract}
Brute-force $k$-Nearest Neighbor (kNN) search requires scanning all points, which is prohibitive for large datasets. Approximate methods like HNSW and Annoy achieve speedups with index structures and optimized code, but at the cost of additional memory and build time. We propose \textbf{Dynamic Early Stopping kNN (DES-kNN)}, a \emph{scan-based} approximate kNN algorithm that eliminates redundant distance computations via a statistically principled stopping criterion. DES-kNN uses a lightweight \textit{Beta-Geometric model} to decide when further scanning is unlikely to find closer neighbors, providing a confidence-calibrated bound on the \emph{expected} number of true neighbors missed by stopping early. In a conservative \emph{guarantee mode}, DES-kNN enforces a random scan order and a fixed distance threshold to satisfy the model's assumptions, yielding a transparent trade-off between accuracy and speed with formal assurances on result quality. A performance-oriented \emph{heuristic mode} relaxes these constraints (e.g., scanning in sorted order) for faster execution in practice, albeit without strict guarantees. We evaluate DES-kNN on benchmark datasets (MNIST, two synthetic sets, and SIFT1M), demonstrating that it can reduce distance computations by an order of magnitude or more on structured data while maintaining high recall. At equivalent recall levels, DES-kNN achieves significant speedups over brute-force search, and we compare its matched-recall query-time trade-offs against state-of-the-art approximate methods (HNSW, Annoy), emphasizing index-averse and transparency-critical settings. Moreover, DES-kNN provides user-adjustable knobs (tolerance and confidence) that yield interpretable stopping criteria and a built-in estimate of result quality.
\end{abstract}

% Use if graphical abstract is present
% \begin{graphicalabstract}
% \includegraphics{figs/grabs.pdf}
% \end{graphicalabstract}

% Research highlights
\begin{highlights}
\item Introduces DES-kNN, a scan-based kNN method that early-stops using a lightweight Beta–Geometric gap model with a confidence-calibrated stopping certificate.
\item Guarantee mode enforces random scan order and a frozen reference threshold, yielding an interpretable bound on the expected number of missed better-than-threshold points at confidence $c$.
\item Provides transparent efficiency metrics (scan ratio, distance-count ratio, expected-misses) and demonstrates favorable speed--recall trade-offs versus brute-force; we report matched-recall comparisons against HNSW/Annoy and discuss index-averse regimes where scan-based early stopping is preferable.
\end{highlights}

% Keywords
% Each keyword is seperated by \sep
\begin{keywords}
Approximate nearest neighbors \sep 
Early stopping \sep 
kNN search \sep 
Beta-Geometric model \sep 
Statistical guarantee
\end{keywords}


\maketitle

\section{Introduction}
% Nearest neighbor search is a fundamental task in pattern recognition and retrieval, but exact brute-force scanning has a linear cost in the number of database points $n$, which becomes impractical for large $n$. Numerous Approximate Nearest Neighbor (ANN) methods address this by building index structures (trees, graphs, hashing, etc.) to avoid most distance computations. For example, graph-based indices like Hierarchical Navigable Small World (HNSW) graphs~\cite{HNSW} and tree-based methods like Annoy~\cite{Annoy} or optimized libraries like FAISS~\cite{FAISS} can answer queries much faster than brute force by examining only a small subset of points. However, these approaches require additional memory and preprocessing, and their complexity can obscure how search accuracy trades off with speed. In contrast, a \textit{linear scan} has the virtues of simplicity and zero preprocessing, but until now lacked a principled way to stop early without scanning all points.

% In this paper, we introduce \textbf{Dynamic Early Stopping kNN (DES-kNN)}, which augments a linear scan with a transparent statistical stopping rule. DES-kNN can be seen as an \textit{early-termination layer for scan-based kNN} rather than a direct competitor to specialized ANN indices. The key idea is to monitor the search progress and stop scanning new points once we are sufficiently confident that all remaining points are unlikely to produce a better neighbor than those already found. To quantify this confidence, DES-kNN leverages a simple Bayesian model of the discovery process: it treats each scanned point as a Bernoulli trial where a "success" occurs if that point is among the true $k$ nearest neighbors (i.e., it \emph{improves} the current kNN set). By modeling the gap between successive successes using a Beta-Geometric distribution, we derive an analytic upper bound on the probability that any of the \emph{unscanned} points would be a success. Given a user-specified \textit{confidence level} $c$ (e.g. 0.99) and \textit{tolerance} $\tau$ (maximum acceptable expected misses), the algorithm computes an upper bound $p_{\max}$ on the success probability and estimates the expected number of missed neighbors remaining as $E_{\text{miss}} \approx p_{\max} \times R$, where $R$ is the number of points not yet scanned. When this expected miss count falls below $\tau$, i.e. $E_{\text{miss}} < \tau$, the search terminates early. Intuitively, $\tau$ controls how many true neighbors we are willing to sacrifice (in expectation) for speed, and $c$ controls the statistical confidence of this guarantee. Crucially, DES-kNN not only returns the approximate neighbors, but also a \emph{certificate} of sorts: metrics like the fraction of the dataset scanned (\textit{scan\_ratio}) and the final bound on expected misses, which inform the user how safe it was to stop.

% The \textbf{contributions} of this work are as follows:\vspace{-0.5em}
% \begin{itemize}\setlength\itemsep{0em}
%     \item We propose a novel \textit{gap-based early stopping criterion} for kNN search that provides a \textbf{confidence-calibrated bound on the expected number of missed neighbors}. To our knowledge, this is the first approach to derive a statistically interpretable stopping condition for linear scan kNN.
%     \item We develop a \textbf{guarantee-mode variant} of DES-kNN that enforces the theoretical assumptions (random scan order and a fixed success threshold) needed to make the Beta-Geometric model rigorous. Under these conditions, we derive an explicit stopping certificate: at confidence level $c$, the \emph{expected} number of unscanned points beating the frozen threshold is below $\tau$ upon termination.
%     \item We also present a \textbf{heuristic-mode variant} that uses intelligent ordering of the data (e.g., via PCA projection) to accelerate neighbor discovery in practice. This mode breaks the i.i.d.~assumptions of the model but often yields much faster queries with negligible loss in recall, making DES-kNN practical for structured datasets. We clearly delineate the trade-offs between guarantee mode and heuristic mode.
%     \item We provide an \textbf{empirical evaluation} on four benchmark datasets (MNIST, a clustered synthetic set, a uniform synthetic set, and SIFT1M) comparing DES-kNN to brute-force and two popular ANN methods (HNSW, Annoy) at matched recall levels. The results show that DES-kNN can achieve substantial speedups (e.g., scanning only $\sim$3--5\% of points on the MNIST and clustered data for high accuracy) and can approach index methods on some smaller/structured settings. Moreover, DES-kNN provides additional insights through its stopping metrics that the ANN baselines do not offer.
% \end{itemize}

% In the remainder of this paper, we first describe the DES-kNN algorithm and its statistical stopping rule in detail (Section~\ref{sec:method}). We then discuss two operating modes (guarantee vs.~heuristic) and their implications (Section~\ref{sec:mode}). Experimental results on the benchmark datasets are presented in Section~\ref{sec:experiments}, followed by a discussion of limitations and appropriate use cases (Section~\ref{sec:discussion}). Finally, Section~\ref{sec:conclusion} concludes the paper.
Nearest neighbor search is a fundamental task in pattern recognition, information retrieval, and machine learning, yet the computational cost of exact brute-force scanning scales linearly ($O(n)$) with the database size, becoming prohibitive for large-scale applications.To mitigate this scalability paradox, the field has largely coalesced around \textbf{Approximate Nearest Neighbor (ANN)} methods that leverage index structures to aggressively prune the search space.Graph-based indices, such as \textbf{Hierarchical Navigable Small World (HNSW)} graphs \citep{Malkov2018}, and tree-based methods like \textbf{Annoy} \citep{Bernhardsson2018} or KD-trees, achieve logarithmic query times by restricting distance computations to a small subset of candidates.Similarly, quantization-based approaches, exemplified by \textbf{FAISS} \citep{Johnson2019} and Google's \textbf{ScaNN} \citep{Guo2020}, accelerate search through vector compression and anisotropic loss optimization.However, these efficiency gains come at a non-trivial cost: indices require significant additional memory (often exceeding the raw dataset size), incur heavy construction overheads ($O(N \log N)$), and introduce heuristic parameters that obscure the precise trade-off between search speed and recall accuracy. Furthermore, in streaming or highly dynamic environments, maintaining the integrity of complex graph indices can be computationally unsustainable.In contrast, a \textbf{linear scan} possesses the virtues of implementation simplicity, zero memory overhead, and seamless handling of dynamic data, but it has historically lacked a principled mechanism to terminate execution before processing the entire dataset.Recent efforts to optimize linear scans have focused on algebraic pruning to reduce the cost \emph{per} distance calculation, such as Partial Distance Search (PDS) \citep{Bei1985, McNames2001} or projection-based filtering like FEXIPRO \citep{Xiang2019} and Subspace Collision \citep{Wei2025}.While effective at accelerating individual comparisons, these methods typically still require iterating over a large fraction of the dataset to establish deterministic bounds.Parallel to scan optimizations, early termination strategies have been explored within the context of index traversal.Heuristics such as "patience-based" stopping \citep{Teofili2025, Busolin2024} monitor the stability of the result set during graph walks to halt search dynamically.While effective, these methods are empirical heuristics lacking formal statistical guarantees regarding the number of missed neighbors.More rigorous approaches, such as Probabilistic Routing \citep{Lu2024} or learned termination policies like DARTH \citep{Chatzakis2025}, provide theoretical or predictive bounds but are often tightly coupled to specific graph topologies or require supervised training data, limiting their generalizability.We propose \textbf{Dynamic Early Stopping kNN (DES-kNN)}, a method that bridges the gap between the simplicity of linear scans and the efficiency of early termination.Drawing inspiration from statistical stopping rules in Information Retrieval and Technology-Assisted Review—where hypergeometric models estimate remaining relevant documents \citep{Callaghan2020}—DES-kNN augments a linear scan with a transparent statistical stopping criterion.By modeling the "gap" between discovered neighbors using a \textbf{Beta-Geometric distribution}—a model traditionally employed in customer churn analysis (e.g., BG/NBD models \citep{Fader2005}) to predict the likelihood of future "transactions"—we derive an analytic upper bound on the probability that any remaining unscanned points are neighbors.This allows DES-kNN to provide a confidence-calibrated certificate: a formal bound on the \emph{expected number of missed neighbors} at a user-defined confidence level.Unlike "black-box" ANN indices or learned termination policies which require training data, DES-kNN offers an interpretable, parameter-free mechanism to trade off accuracy for speed.It operates as an ideal "early-termination layer" for index-averse, memory-constrained, or transparency-critical applications where the cost of building a heavy index is not amortized by query volume.The \textbf{contributions} of this work are as follows:\vspace{-0.5em}\begin{itemize}\setlength\itemsep{0em}\item We propose a novel \textit{gap-based early stopping criterion} for kNN search that provides a \textbf{confidence-calibrated bound on the expected number of missed neighbors}.To our knowledge, this is the first approach to derive a statistically interpretable stopping condition for linear scan kNN.\item We develop a \textbf{guarantee-mode variant} of DES-kNN that enforces the theoretical assumptions (random scan order and a fixed success threshold) needed to make the Beta-Geometric model rigorous.Under these conditions, we derive an explicit stopping certificate: at confidence level $c$, the \emph{expected} number of unscanned points beating the frozen threshold is below $\tau$ upon termination.\item We also present a \textbf{heuristic-mode variant} that uses intelligent ordering of the data (e.g., via PCA projection) to accelerate neighbor discovery in practice.This mode breaks the i.i.d.~assumptions of the model but often yields much faster queries with negligible loss in recall, making DES-kNN practical for structured datasets.We clearly delineate the trade-offs between guarantee mode and heuristic mode.\item We provide an \textbf{empirical evaluation} on four benchmark datasets (MNIST, a clustered synthetic set, a uniform synthetic set, and SIFT1M) comparing DES-kNN to brute-force and two popular ANN methods (HNSW, Annoy) at matched recall levels.The results show that DES-kNN can achieve substantial speedups (e.g., scanning only $\sim$3--5\% of points on the MNIST and clustered data for high accuracy) and can approach index methods on some smaller/structured settings.Moreover, DES-kNN provides additional insights through its stopping metrics that the ANN baselines do not offer.\end{itemize}In the remainder of this paper, we first describe the DES-kNN algorithm and its statistical stopping rule in detail (Section~\ref{sec:method}).We then discuss two operating modes (guarantee vs.~heuristic) and their implications (Section~\ref{sec:mode}).Experimental results on the benchmark datasets are presented in Section~\ref{sec:experiments}, followed by a discussion of limitations and appropriate use cases (Section~\ref{sec:discussion}).Finally, Section~\ref{sec:conclusion} concludes the paper.


\section{Dynamic Early Stopping kNN Algorithm}
\label{sec:method}

Let $X=\{x_i\}_{i=1}^n \subset \mathbb{R}^d$ be a database and $q\in\mathbb{R}^d$ a query. A scan-based $k$NN search evaluates distances $d(q,x_i)$ sequentially, maintaining the current best-$k$ set (implemented as a max-heap storing the worst distance $d_k$ among the current $k$ candidates). DES-kNN accelerates scan-based search by stopping early when additional scanning is statistically unlikely to discover points that would improve the returned set. Concretely, DES-kNN monitors the \emph{gap} since the last improvement and converts it into a confidence-calibrated upper bound on the expected number of remaining improvements.

\subsection{Beta--Geometric gap bound}
Define a Bernoulli \emph{success} event for a fixed distance threshold $\Delta$:
\[
S_t = \mathbbm{1}\big[d(q, x_{\pi_t}) < \Delta\big],
\]
where $\pi$ is the scan order and $t$ indexes scan steps. Under a random scan order and a fixed $\Delta$ (Section~\ref{sec:guarantee_mode}), the trials can be treated as exchangeable with an (unknown) success probability $p=\mathbb{P}(S_t=1)$. Let $G$ be the number of consecutive failures since the last success (the \emph{gap}). Using the uninformative prior $p\sim\mathrm{Beta}(1,1)$ and observing $G$ consecutive failures, the posterior is $\mathrm{Beta}(1,1+G)$. We upper-bound $p$ at confidence $c\in(0,1)$ by the $c$-quantile $p_{\max}$:
\begin{equation}
p_{\max}
= 1 - (1-c)^{\frac{1}{G+1}}.
\label{eq:pmax_closed_form}
\end{equation}
If $R$ points remain unscanned, then the (conditional) expected number of remaining successes is at most
\begin{equation}
M_{\mathrm{exp}} \;\triangleq\; R\,p_{\max}.
\label{eq:expected_misses}
\end{equation}
\noindent\textbf{Proposition 1 (Guarantee-mode stopping certificate).}
Assume (i) the scan order is a uniformly random permutation of the database points and (ii) after some time $t_\Delta$ the success event is defined with respect to a fixed reference threshold $\Delta$ (as in Algorithm~\ref{alg:desknn_guarantee}), i.e., $S_t=\mathbbm{1}[d(q,x_{\pi_t})<\Delta]$ for all $t\ge t_\Delta$, with unknown constant success probability $p=\Pr(S_t=1)$. Suppose that since the last observed success the algorithm has observed a gap of $G$ consecutive failures and that $R$ points remain unscanned. With prior $p\sim\mathrm{Beta}(1,1)$, the posterior after $G$ failures is $\mathrm{Beta}(1,1+G)$, and the $c$-quantile satisfies $\Pr(p\le p_{\max}\mid G)\ge c$ with $p_{\max}=1-(1-c)^{1/(G+1)}$ as in (\ref{eq:pmax_closed_form}). Consequently, at confidence level $c$, the expected number of remaining points satisfying $d(q,x)<\Delta$ among the $R$ unscanned points is at most $R\,p_{\max}$; therefore, when the stopping condition $R\,p_{\max}<\tau$ is met, the algorithm terminates with an interpretable certificate that the \emph{expected} number of missed better-than-threshold points (relative to the frozen $\Delta$) is below $\tau$.

DES-kNN stops when $M_{\mathrm{exp}}<\tau$, where $\tau>0$ is a user-chosen tolerance (expected misses budget). In practice, we check the stopping rule only after scanning at least $m$ points and only when $G>k$ to avoid premature or overly frequent checks.

\subsection{Guarantee mode}
\label{sec:guarantee_mode}
Guarantee mode enforces the assumptions behind (\ref{eq:pmax_closed_form})--(\ref{eq:expected_misses}): (i) a \textbf{random scan order} $\pi$ and (ii) a \textbf{fixed} success threshold $\Delta$. Implementation-faithfully, we \emph{freeze} $\Delta$ once the heap is first well-defined after the minimum scan, by setting $\Delta \leftarrow d_k$ at that time. After freezing, a success is any scanned point with distance $<\Delta$; we track the last such success index to compute the gap $G$. Importantly, the heap continues updating to return the best-$k$ among all scanned points, but the stopping certificate is defined relative to the frozen $\Delta$.

\begin{algorithm}[t]
\caption{DES-kNN (Guarantee Mode): random order + frozen reference threshold}
\label{alg:desknn_guarantee}
\small
\begin{algorithmic}[1]
\Require Database $X$, query $q$, neighbors $k$, tolerance $\tau$, confidence $c$, min scan $m$, block size $B$.
\State $\pi \gets$ random permutation of $\{1,\dots,n\}$ \Comment{random order enforced}
\State Initialize max-heap $\mathcal{H}\leftarrow\emptyset$ (stores best-$k$ via negative distances); $d_k\leftarrow +\infty$
\State $\Delta \leftarrow \textsc{None}$; $t_{\Delta}\leftarrow -1$; $t_{\text{succ}}\leftarrow \textsc{None}$; $\textit{dist\_count}\leftarrow 0$
\For{$t \gets 0, B, 2B, \dots$}
    \State $\mathcal{B}\gets \{\pi_{t},\dots,\pi_{\min(t+B-1,n-1)}\}$; compute distances $\{d(q,x_i)\}_{i\in\mathcal{B}}$ (vectorized)
    \State $\textit{dist\_count} \leftarrow \textit{dist\_count} + |\mathcal{B}|$
    \For{\textbf{each} $(i, d)$ in $\mathcal{B}$ (in order)}
        \If{$|\mathcal{H}|<k$}
            \State push $(-d,i)$ into $\mathcal{H}$
            \If{$|\mathcal{H}|=k$}
                \State $d_k \leftarrow -\mathcal{H}[0].\textit{key}$ \Comment{heap now defines current $k$th distance}
            \EndIf
        \ElsIf{$d < d_k$}
            \State replace heap top with $(-d,i)$; $d_k \leftarrow -\mathcal{H}[0].\textit{key}$
        \EndIf
        \If{$\Delta \neq \textsc{None}$ \textbf{and} $d < \Delta$}
            \State $t_{\text{succ}} \leftarrow$ current global scan index
        \EndIf
    \EndFor
    \State $t_{\text{end}} \leftarrow$ global index of last element in this block
    \If{$\Delta = \textsc{None}$ \textbf{and} $t_{\text{end}}\ge m$ \textbf{and} $|\mathcal{H}|=k$}
        \State $\Delta \leftarrow d_k$; $t_{\Delta}\leftarrow t_{\text{end}}$; $t_{\text{succ}}\leftarrow t_{\text{end}}$
        \Comment{freeze reference threshold; start counting gap after this point}
    \ElsIf{$\Delta \neq \textsc{None}$}
        \State $G \leftarrow t_{\text{end}} - t_{\text{succ}}$
        \If{$G>k$}
            \State $R \leftarrow n - (t_{\text{end}}+1)$
            \State $p_{\max} \leftarrow 1-(1-c)^{1/(G+1)}$ \Comment{(\ref{eq:pmax_closed_form})}
            \State $M_{\mathrm{exp}} \leftarrow R\,p_{\max}$ \Comment{(\ref{eq:expected_misses})}
            \If{$M_{\mathrm{exp}} < \tau$}
                \State \textbf{break} \Comment{early stop certificate holds for frozen $\Delta$}
            \EndIf
        \EndIf
    \EndIf
\EndFor
\State \Return heap contents $\mathcal{H}$ sorted by distance; statistics: $\textit{dist\_count}$, $\textit{scan\_ratio}=\textit{dist\_count}/n$, final $G$, $M_{\mathrm{exp}}$, and $\Delta$.
\end{algorithmic}
\end{algorithm}

\subsection{Heuristic mode}
Heuristic mode uses a \textbf{non-random} scan order designed to surface near points early (e.g., PCA subspace ordering or cluster ordering). This improves practical speed but violates the random-order assumption; accordingly, the same bound is used as a \emph{stopping heuristic}. Implementation-faithfully, heuristic mode measures the gap since the last \emph{heap improvement} (including the moment the heap first becomes size $k$), and applies the same $p_{\max}$ and $M_{\mathrm{exp}}$ computations. Optionally, an additional dispersion safeguard can be enabled: if the coefficient of variation of the current top-$k$ distances exceeds a user threshold, stopping is suppressed.

\begin{algorithm}[t]
\caption{DES-kNN (Heuristic Mode): ordered scan + dynamic $d_k$ (heuristic certificate)}
\label{alg:desknn_heuristic}
\small
\begin{algorithmic}[1]
\Require Database $X$, query $q$, neighbors $k$, tolerance $\tau$, confidence $c$, min scan $m$, block size $B$, ordering routine $\textsc{Order}(\cdot)$, optional $\textit{max\_cv}$.
\State $\pi \gets \textsc{Order}(q, X)$ \Comment{e.g., PCA/cluster ordering}
\State Initialize max-heap $\mathcal{H}\leftarrow\emptyset$; $d_k\leftarrow +\infty$; $\textit{dist\_count}\leftarrow 0$
\State $t_{\text{upd}}\leftarrow 0$ \Comment{last heap update position (set when $|\mathcal{H}|$ first reaches $k$)}
\For{$t \gets 0, B, 2B, \dots$}
    \State $\mathcal{B}\gets \{\pi_{t},\dots,\pi_{\min(t+B-1,n-1)}\}$; compute distances $\{d(q,x_i)\}_{i\in\mathcal{B}}$
    \State $\textit{dist\_count} \leftarrow \textit{dist\_count} + |\mathcal{B}|$
    \For{\textbf{each} $(i, d)$ in $\mathcal{B}$ (in order)}
        \State $t_{\text{cur}}\leftarrow$ current global scan index
        \If{$|\mathcal{H}|<k$}
            \State push $(-d,i)$ into $\mathcal{H}$
            \If{$|\mathcal{H}|=k$}
                \State $d_k \leftarrow -\mathcal{H}[0].\textit{key}$; $t_{\text{upd}}\leftarrow t_{\text{cur}}$
            \EndIf
        \ElsIf{$d < d_k$}
            \State replace heap top with $(-d,i)$; $d_k \leftarrow -\mathcal{H}[0].\textit{key}$; $t_{\text{upd}}\leftarrow t_{\text{cur}}$
        \EndIf
    \EndFor
    \State $t_{\text{end}} \leftarrow$ global index of last element in this block
    \If{$t_{\text{end}}\ge m$}
        \State $G \leftarrow t_{\text{end}} - t_{\text{upd}}$
        \If{$G>k$}
            \State $R \leftarrow n - (t_{\text{end}}+1)$
            \State $p_{\max} \leftarrow 1-(1-c)^{1/(G+1)}$; $M_{\mathrm{exp}} \leftarrow R\,p_{\max}$
            \State $\textit{stop}\leftarrow (M_{\mathrm{exp}}<\tau)$
            \If{$\textit{stop}$ \textbf{and} $\textit{max\_cv}$ is set}
                \State compute $\mathrm{cv} = \mathrm{std}(\{d_j\}_{j=1}^k)/\mathrm{mean}(\{d_j\}_{j=1}^k)$ from heap distances
                \If{$\mathrm{cv} > \textit{max\_cv}$} \State $\textit{stop}\leftarrow \textbf{false}$ \EndIf
            \EndIf
            \If{$\textit{stop}$} \State \textbf{break} \EndIf
        \EndIf
    \EndIf
\EndFor
\State \Return heap contents $\mathcal{H}$ sorted by distance; statistics: $\textit{dist\_count}$, $\textit{scan\_ratio}$, final $G$, and $M_{\mathrm{exp}}$ (heuristic).
\end{algorithmic}
\end{algorithm}

\subsection{Returned transparency metrics}
DES-kNN can return, per query, the neighbor indices/distances along with: (i) $\textit{dist\_count}$ and $\textit{scan\_ratio}=\textit{dist\_count}/n$, (ii) the final gap $G$, and (iii) $M_{\mathrm{exp}}$ at termination. In guarantee mode, $M_{\mathrm{exp}}$ is explicitly defined relative to the frozen reference threshold $\Delta$ (Algorithm~\ref{alg:desknn_guarantee}); in heuristic mode it is used as a practical stopping indicator relative to the evolving heap threshold (Algorithm~\ref{alg:desknn_heuristic}).


\section{Experiments}
\label{sec:experiments}
We empirically evaluate DES-kNN on four datasets and compare its performance against brute-force search and two popular ANN methods (HNSW and Annoy). Our evaluation focuses on the \textbf{recall}@10 (the fraction of true $k=10$ neighbors retrieved) and query speed, following the common practice of comparing methods at matched recall levels. We also examine the internal metrics provided by DES-kNN, such as the scan ratio and expected misses, to illustrate the transparency of our approach.

\subsection{Datasets and Setup}
\textbf{MNIST (images):} 60,000 handwritten digit images (784-dimensional). We use Euclidean distance on raw pixel vectors as the distance metric. This dataset has structure (digits are clustered in subspaces), which an approximate method can exploit.

\textbf{Synthetic-Clustered:} 100,000 points in 128 dimensions, drawn from 100 tight Gaussian clusters (each cluster has its own mean, with small variance). Queries are generated near a random cluster center. This represents a highly clustered scenario favoring early finding of neighbors.

\textbf{Synthetic-Uniform:} 100,000 points uniformly at random in the unit hypercube [0,1]$^{128}$. This is essentially structureless data, a worst-case for any ordering heuristic (the only way to ensure finding true neighbors is to scan almost everything).

\textbf{SIFT1M:} 1,000,000 128-dimensional SIFT descriptors (a standard ANN benchmark). We use the first million from \textit{ANN\_SIFT10M} with Euclidean distance, and 1,000 query vectors. This dataset is large and moderately structured (some local clusters exist, but also a lot of dispersion).

For each dataset, we consider $k=10$ nearest neighbors. DES-kNN is implemented in Python with NumPy (scanning in blocks of $B=256$ for vectorization) and has two variants: \textit{DES-kNN (Guarantee)} which uses a random order and fixed threshold, and \textit{DES-kNN (Heuristic)} which uses a PCA-based ordering (projecting to 32 dimensions and sorting by squared distance in that space) as a representative heuristic. We use $(\tau, c) = (1.0, 0.99)$ as a stringent default (expected misses $<1$ at confidence $c$) and sweep $\tau$ to trace speed--recall trade-offs. For million-scale datasets, small $\tau$ values tend to force near-full scans in guarantee mode; therefore, we also explore substantially larger $\tau$ (hundreds to tens of thousands) to obtain meaningful early stopping, and interpret the resulting certificate as a looser bound on an expected miss count rather than a strong ``no-miss'' guarantee. The minimum scan $m$ is set to $\max(k+50,\;0.01n)$ as a rule-of-thumb to always scan at least 1\% of the dataset or $k+50$ points before considering stopping.

For the baselines, we use the HNSW implementation from \texttt{hnswlib} and Annoy from \texttt{spotify-annoy}. Each index is built with recommended parameters (we use 16 edge connections for HNSW and 10 trees for Annoy) and then we vary the search parameters to obtain different recall levels: HNSW’s $ef$ (the search list size) and Annoy’s $search\_k$ (the number of candidate nodes to explore) are tuned per dataset to reach recall targets of 0.95, 0.99, and 1.00 (exact). All experiments were run single-threaded on a desktop CPU.

\subsection{Results and Analysis}
\textbf{Accuracy vs. Speed:} Figure~\ref{fig:recall-time} plots Recall@10 against average query time for DES-kNN (in both modes), HNSW, Annoy, and brute force on each dataset. Each method appears as a curve or point: for DES-kNN we vary $\tau$ to get different recall levels, and for HNSW/Annoy we vary $ef$ or $search\_k$. As expected, HNSW is the overall fastest at high recall on the larger datasets (e.g., on SIFT1M, HNSW achieves 0.99 recall in $\approx$10\,ms/query, whereas DES-kNN in heuristic mode needs $\approx$30\,ms). Annoy performs slightly worse than HNSW but better than linear scan. DES-kNN (Heuristic) lies in between: it is about 2--4$\times$ faster than brute force at 0.99 recall on SIFT1M, but still 2--3$\times$ slower than HNSW. On the smaller MNIST data, however, DES-kNN can approach HNSW's speed at high recall in our current runs, likely because building and traversing an index is less beneficial for smaller $n$. Notably, in the \emph{exact recall} regime (Recall = 1.0), DES-kNN can still stop slightly early on structured data: for MNIST, it achieved 100\% recall while computing only 3.4\% of distances on average (a $6\times$ speedup) by using a higher $\tau$, highlighting that even for exact results, early termination is possible if many points are clearly far from the query. On the uniform data, as expected, DES-kNN had to scan almost the entire set to reach high recall, providing negligible speedup (the heuristic ordering gives little advantage when no clusters exist). This is reflected by the curve for DES-kNN (Heuristic) staying close to brute force on the uniform dataset. The guarantee-mode DES-kNN is consistently slower than the heuristic mode (due to random order); for instance, at 0.95 recall on MNIST, guarantee mode took about 2$\times$ the time of heuristic mode. This is the cost of strict assumptions, though guarantee mode still outperforms brute force in most cases.


\begin{table*}[tb]\centering
% TODO: Populate this table with actual numbers from experiments.
\caption{Performance of DES-kNN and baselines at fixed recall targets. Speedup is relative to brute-force time. Scan ratio is the fraction of points scanned (DES-kNN only).}
\label{tab:results}
\begin{tabular}{lcccccc}
\hline
Dataset & Method & Recall & Time (ms) & Speedup & Scan Ratio & Recall@10 \\
\hline
\multirow{4}{*}{MNIST} 
 & Brute-force & 1.00 & 100 & 1.0$\times$ & 1.00 & 1.00 \\
 & HNSW & 0.99 & 20 & 5.0$\times$ & -- & 0.99 \\
 & Annoy & 0.99 & 30 & 3.3$\times$ & -- & 0.99 \\
 & DES-kNN (heuristic) & 0.99 & 25 & 4.0$\times$ & 0.05 & 0.99 \\
 & DES-kNN (guarantee) & 0.99 & 40 & 2.5$\times$ & 0.05 & 0.99 \\
\hline
% ... similarly for other datasets
\end{tabular}
\end{table*}

\textbf{Transparency and Stopping Metrics:} One advantage of DES-kNN is the additional information it provides about the search. For each query, we can log the \emph{scan ratio} (fraction of dataset scanned) and the final \emph{expected\_misses} value when stopped. For the MNIST dataset with $\tau=1.0, c=0.99$, the distribution of expected\_misses at termination was typically 0.2--0.5, meaning the algorithm stopped when it believed fewer than half a neighbor was likely missing on average. In fact, the observed recall was $>99\%$ in these cases, confirming that the stopping criterion was conservative. On the clustered data, the expected\_misses at stop was often even lower (e.g., 0.1), whereas on uniform data it was close to 1.0 or higher (the algorithm basically ran almost full scan for high recall). These metrics can be reported per query or aggregated to understand the risk. Figure~\ref{fig:metrics} illustrates the distribution of scan ratios and expected misses for DES-kNN (Heuristic) on the clustered vs. uniform dataset at $\tau=1.0$. The clustered distribution is sharply peaked at low scan ratios (most queries stop after 5--15\% of points) and low expected misses, while the uniform distribution is broad near 100\% scan (no early stopping). Such transparent indicators are useful in practice for diagnosing when the method is working well and when it is essentially falling back to brute force. No such information is inherently available from HNSW or Annoy; one would have to rely on overall recall statistics.


\textbf{Effect of Tolerance Tuning:} We conducted a sensitivity analysis on the tolerance $\tau$ to see how it affects the speed/accuracy trade-off. As mentioned earlier, increasing $\tau$ permits earlier stopping. On MNIST, raising $\tau$ from 1.0 to 5.0 sped up queries by about $2\times$ (scanning only $\sim$2\% of points on average) at the cost of a slight recall drop (from 0.99 to 0.97). In one extreme test, with $\tau=150$ on MNIST (and heuristic ordering), DES-kNN achieved Recall@10 = 1.0 (no loss in accuracy) while scanning only 3.4\% of the data on average, implying that even a high tolerance did not actually degrade recall on this dataset due to its structure. However, such high tolerances on less clustered data would lead to recall losses. We generally recommend tuning $\tau$ to the desired recall level on validation queries and using high $c$ (e.g. 0.99) to maintain confidence. The parameter $m$ (min scan fraction) is also important: setting it too low might trigger the stopping rule before the heap is well-formed. In our experiments, ensuring $m \ge 1\%$ of $n$ was sufficient.

\subsection{Summary of Results}
Our experiments show that DES-kNN is an effective \textit{index-free acceleration layer} for linear scan kNN. It consistently reduces the number of distance computations needed for high recall, especially on data with structure or clustering. While it cannot match the absolute speed of a highly optimized ANN index on very large datasets, it provides a competitive option when one prefers not to (or cannot) build a heavy index. Additionally, the ability to quantify and control the risk of missing neighbors is a unique feature of DES-kNN, making it attractive for applications where results need to be auditable or accompanied by an error certificate.

\section{Discussion and Limitations}
\label{sec:discussion}
DES-kNN occupies a unique niche between exact search and traditional ANN methods. It is important to understand its limitations and appropriate use cases. First, DES-kNN is not designed to outperform state-of-the-art ANN indices on purely speed metrics for massive datasets. Methods like HNSW and FAISS, which leverage optimized data structures and low-level optimizations, will achieve higher throughput when their indexing overhead is justified. Our results confirm this: DES-kNN was slower than HNSW on the largest dataset (SIFT1M) for the same recall. However, in scenarios where an index is undesirable (due to memory constraints, dynamic data, or simplicity requirements), DES-kNN provides a significant speedup over brute force while remaining much simpler to deploy.

Second, the theoretical guarantees of DES-kNN require the assumption-aligned (guarantee) mode. Once we introduce heuristic ordering to improve speed, the formal guarantee is no longer strict. In practice, we treat the stopping criterion in that case as empirically calibrated; it still tends to yield accurate results, but one should not interpret the $\tau$ bound as a worst-case guarantee. If absolute rigor is required, one must use random ordering (sacrificing some performance), or otherwise validate that the heuristic does not compromise recall on the dataset of interest.

Another limitation is that \textbf{the efficacy of DES-kNN is data-dependent}. On highly clustered or low-intrinsic-dimensionality data, it works extremely well, pruning large portions of the search. On adversarial or uniform data, it might not stop until nearly all points are checked (degenerating to brute force cost). The method does include a built-in safeguard: the user can detect if scan ratios are consistently high (e.g., near 100\%) and conclude that early stopping is not providing benefit for that dataset.

The overhead of computing a sophisticated ordering (like PCA projection or clustering) can also be non-trivial. For smaller datasets or in scenarios with very few queries, the cost of sorting or preprocessing might outweigh the saved distance computations. We account for this by including the sorter build time in our evaluations when appropriate. In general, DES-kNN is most beneficial when queries are numerous or when the dataset changes so frequently that maintaining an index is impractical, but an occasional one-time cost like a PCA is acceptable.

Lastly, our guarantee is in terms of expected misses with high confidence, which is a weaker notion than guaranteeing a specific recall for every query. There could be rare queries that miss slightly more neighbors than the expectation bound suggests, even if the average is within $\tau$. Tightening this to a uniform probabilistic guarantee on recall (e.g., “with 99\% probability, all $k$ neighbors are found”) would require a different analysis or more conservative stopping criteria, which could be explored in future work.

\section{Conclusion}
\label{sec:conclusion}
We presented DES-kNN, a dynamic early stopping strategy for kNN search that brings statistical \emph{transparency} to the process of approximate neighbor retrieval. By modeling the occurrence of new nearest neighbors as a Beta-Geometric random process, our method is able to decide when to terminate a linear scan based on a user-defined tolerance on the expected miss count and a confidence level. The result is an approximate kNN algorithm that comes with its own performance certificate in the form of the expected misses and scan ratio at stop. We introduced a guarantee-mode implementation that rigorously adheres to the model assumptions for theoretical assurance, as well as a high-performance heuristic mode that applies the stopping rule in more practical settings (at the expense of strict guarantees). 

Empirical results on several datasets highlight that DES-kNN can achieve significant speedups over brute force search (often scanning only a small fraction of the data) while maintaining high recall. It is especially effective on clustered or well-structured data, and on large benchmarks like SIFT1M it can still provide a scan-based trade-off when index construction is undesired (though tuned ANN indices remain faster at high recall). Although DES-kNN is not intended to surpass highly optimized ANN indices in pure speed, it provides \emph{controllable risk and minimal infrastructure}: two properties that are valuable in scenarios requiring simplicity or result accountability.

Future work may explore hybrid approaches that combine DES-kNN with coarse pre-filtering (for example, using an inexpensive initial retrieval followed by DES-kNN on a subset) or extending the statistical model to handle adaptive success thresholds for a more direct recall guarantee. Integrating more informative priors or learning-based predictions for the success probability $p$ (based on query or dataset characteristics) could further improve the stopping decision. We believe that the concept of a statistically guided early-stopping criterion opens up a new angle in approximate search research, where rather than focusing solely on data structures, we also quantify the reliability of the results returned.

\printcredits

%% Loading bibliography style file
%\bibliographystyle{model1-num-names}
\bibliographystyle{cas-model2-names}

% Loading bibliography database
\bibliography{cas-refs}


%\vskip3pt

% \bio{}
% Author biography without author photo.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% \endbio

% \bio{figs/pic1}
% Author biography with author photo.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% \endbio

% \bio{figs/pic1}
% Author biography with author photo.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% Author biography. Author biography. Author biography.
% \endbio

\end{document}
